이번시간에는 BOW의 TF-IDF(Term Frequency - Inverse Document Frequency)에 대해 알아 보겠습니다.

이전에 설명한 카운트 기반 벡터화는 숫자가 높을수록 중요한 단어로 인식합니다. 하지만, 단순히 단어의 빈도만 고려한다면 모든 문서에서 자주 쓰일 수 밖에 없는 단어들이 중요하다고 인식 될 수 있습니다. 이런 문제를 보완하기 위해 TF-IDF 벡터화를 사용합니다.

TF-IDF는 개별 문서에서 자주 등장하는 단어에는 높은 가중치를, 모든 문서에서 자주 등장하는 단어에 대해서는 패널티를 주는 방식으로 값을 부여합니다. 예를 들어 총 5개의 문서가 있다고 가정하면, 딥러닝이라는 단어는 5개 문서에서 모두 등장하고, 머신러닝이라는 단어는 1번 문서에서만 빈번히 등장한다고 했을 때, TF-IDF의 값은 딥러닝이라는 단어는 낮게 부여 되고 머신러닝이라는 단어는 높게 부여 됩니다.

문서의 양이 많을 경우 보통 TF-IDF 방식의 벡터화를 사용합니다.

TF-IDF의 사용방법은 CounterVectorizer와 크게 다르지 않습니다.

from sklearn.feature_extraction.text import TfidVectorizer

    'This is the first document.',

    'This is the second second document.',

    'And the third one.'

]

tfidf = TfidfVectorizer()
print(tfidf.fit_transform(corpuse).toarray() #corpus 리스트 네부 텍스트로부터 각 TF-IDF를 계산합니다.
print(tfidf.vocabulary_) #각 단어의 인덱스가 어떻게 부여되었는지 보여줍니다.
#output : 

[[0.         0.43306685 0.56943086 0.43306685 0.         0.

  0.33631504 0.         0.43306685]

 [0.         0.30833187 0.         0.30833187 0.         0.81083871

  0.2394472  0.         0.30833187]

 [0.54645401 0.         0.         0.         0.54645401 0.

  0.32274454 0.54645401 0.        ]]

{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}



출처: https://carpe08.tistory.com/182 [이서]
