이전 시간에 말했듯이, 다중공선성을 해결하는 방법은 크게 세가지가 있다.

1. 변수 정규화
2. 변수 제거
3. PCA(주성분 분석)

변수 정규화 방법은 수치형 데이터들을 Min-Max scaling 이나 Z-Score scaling 등의 기법으로 정규화 시켜주는 방법이고, 변수 제거 방법은 변수의 VIF(분산팽창요인) 계수가 10이상인 변수를 제거하는 방법이였다. 

마지막 세번째 방법은 PCA를 통한 해결 방법이다. PCA를 이해하기 위해서 먼저 차원축소의 개념을 이해해야한다.

차원 축소란?
차원 축소는 많은 피처로 구성된 다차원 데이터 셋의 차원을 축소해 새로운 차원의 데이터 셋을 생성하는 것이다.
일반적으로 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고, 희소(sparse)한 구조를 가지게 된다.
수백 개 이상의 피처로 구성된 데이터 셋의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어진다.
또한 피처가 많은 경우 개별 피처간의 상관관계가 높을 가능성이 크다.

선형회귀와 같은 선형 모델에서는 입력 변수간의 상관관계가 높을 경우, 이로 인한 다중공성선 문제로 모델의 예측 성능이 떨어진다.
그리고 수십 개 이상의 피처가 있는 데이터의 경우 이를 시각적으로 표현해 데이터의 특성을 파악하기는 불가능하다.
이 경우 3차원 이하의 피처가 있는 데이터의 경우 이를 시각적으로 데이터를 압축해서 표현할 수 있다.
또한 차원 축소를 할 경우 학습 데이터의 크기가 줄어들어서 학습에 필요한 처리 능력도 향상 시킬 수 있다.

일반적으로 차원 축소는 feature selection 과 feature extraction으로 나눌 수 있다.
feature selection 은 말 그대로 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것이다.
feature extraction은 기존 피처를 저차원의 중요 피쳐로 압축해서 추출하는 것이다.
새롭게 추출된 중요 특성은 기존의 피처가 압축된것이므로 기존의 피처와는 완전히 다른 값이 된다.
다중공선성을 해결하는 방법 중 하나인 PCA인 경우 feature extraction의 기법 중 하나이다.

feature extraction�

기존 피처를 단순 압축이 아닌, 함축적으로 더 잘 설명할 수 있는 또 다른 공간으로 매핑해 추출하는 것이다.
학생 을 평가하는 다양한 요소로 모의고사 성적, 종합 내신 성적, 수능성적 등 관련된 여러 가지 피처로 되어있는 데이터 셋이라면, 학업 성취도, 커뮤니케이션 능력, 문제 해결력 과 같은 더 함축적인 요약 특성으로 추출할 수 있다.
