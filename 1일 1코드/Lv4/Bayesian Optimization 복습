튜닝 부분에서 Randomforest, XGBoost, Light GBM 총 3개의 모델을 튜닝하고 Voting Classifier로 만드는 과정을 진행 할 예정이다.

모델을 튜닝 하기전에 우선 진행했던 Bayesian Optimization에 대해 복습하는 시간을 갖도록 하겠습니다.

Bayesian Optimization

우리가 흔히 알고 있는 하이퍼 파라미터 튜닝방법은 Grid Search, Random Search 이다.
Grid Search란 가능한 하이퍼파라미터 경우의 수를 일정 구간으로 나눠 구간별로 균일하게 대입해보는 방식이다.
간격을 어떻게 잡을지 정하는 것이 문제고, 시간이 너무 오래걸린다는 단점이 있다.

Random Search란 가능한 하이퍼파라미터 조합을 random하게 선택해서 대입해보는 방식이다.
위 두가지 튜닝방법의 문제점은 이제까지의 사전지식(실험결과)이 반영되지 않는 다는 것이다.
 
Baysian Optimization은 사전지식(실험결과)을 반영해가며 하이퍼파라미터를 탐색한다. 즉, 현재까지 얻어진 모델의 파라미터와 추가적인 실험 정보를 통해 데이터가 주어졌을때, 모델의 성능이 가장 좋은 확률이 높은 파라미터를 찾아낸다.
    
